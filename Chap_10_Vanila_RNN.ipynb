{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap 10_Vanila_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDbtzi1Fex9aFbjHCqZCIC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaheeKo/Deep-learning-study/blob/main/Chap_10_Vanila_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIaeo-Y9vLg8"
      },
      "source": [
        "**1. Numpy로 구현해보기**\n",
        "\n",
        "< 방법 >\n",
        "\n",
        "hidden_state_t 초기 은닉 상태를 0벡터로 초기화\n",
        "\n",
        "각 시점마다 input_t을 받아서\n",
        "\n",
        "output_t: 입력과 은닉상태를 가지고 연산(tanh)\n",
        "\n",
        "계산결과 -> 출력이면서, 다시 현재 시점의 은닉상태가 됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJRtqpWbOOJ_"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "timesteps = 10 # 시점의 수, NLP에서 보통 문장의 길이\n",
        "input_size = 4 # 입력의 차원, NLP에서 보통 단어 벡터의 차원 \n",
        "hidden_size = 8 # 은닉 상태의 크기, 메모리 셀의 용량\n",
        "\n",
        "inputs = np.random.random((timesteps, input_size)) # 입력에 해당, 2D 텐서\n",
        "hidden_state_t = np.zeros((hidden_size,)) # 초기 은닉 상태 초기화\n",
        "\n",
        "Wx = np.random.random((hidden_size, input_size)) # 입력에 대한 가중치\n",
        "Wh = np.random.random((hidden_size, hidden_size)) # 은닉상태에 대한 가중치\n",
        "b = np.random.random((hidden_size,)) # 편향\n",
        " # => 출력은 (hidden_size,)로 나옴\n",
        "\n",
        "total_hidden_state = []\n",
        "\n",
        "for input_t in inputs:\n",
        "  output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b)\n",
        "  total_hidden_state.append(output_t)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoRSpxeqfW0c"
      },
      "source": [
        "**2. 파이토치의 nn.RNN()로 구현**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2AxbLbBfTfs",
        "outputId": "38c8181d-c441-4d5d-ead6-27258afd1a02"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        " # 하이퍼파라미터 정의\n",
        "input_size = 5 # 입력의 크기\n",
        "hidden_size = 8 # 은닉 상태의 크기\n",
        "\n",
        " # 입력 텐서 정의\n",
        "inputs = torch.Tensor(1, 10, 5)  # (batch_size, time_steps, input_size)\n",
        "\n",
        " # RNN 셀 만들기\n",
        "  # RNN 셀은 두 개의 입력 리턴: (모든 시점의 은닉 상태들, 마지막 시점의 은닉 상태)\n",
        "cell = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "outputs, _status = cell(inputs) # 모든 시점의 은닉 상태들, 마지막 시점의 은닉 상태 \n",
        "print(outputs.shape) # : torch.Size([1, 10, 8]) = 10번의 시점 동안 8차원의 은닉상태가 출력\n",
        "print(_status.shape) # : torch.Size([1, 1, 8]) = 마지막 시점의 은닉상태 크기\n",
        "\n",
        "print('---------------------------')\n",
        "# 깊은 순환 신경망\n",
        "cell_D = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True) # 은닉층이 2개이도록 num_layers = 2\n",
        "outputs, _status = cell_D(inputs)\n",
        "print(outputs.shape) # : torch.Size([1, 10, 8]) = 마지막 층의 모든 시점의 은닉상태라서 동일 \n",
        "print(_status.shape) # : torch.Size([2, 1, 8]) = 마지막 시점의 은닉상태 (층의 개수, 배치 크기, 은닉상태의 크기)\n",
        "\n",
        "print('---------------------------')\n",
        "# 양방향 순환 신경망\n",
        "cell_B = cell = nn.RNN(input_size = 5, hidden_size = 8, num_layers = 2, batch_first=True, bidirectional = True) # 은닉층 2개 & 양방향(bidirectional = True)\n",
        "outputs, _status = cell_B(inputs)\n",
        "print(outputs.shape) # : torch.Size([1, 10, 16]) = (배치크기, 시퀀스 길이, 은닉상태 크기x2) <= 양방향의 은닉상태가 연결돼서\n",
        "print(_status.shape) # : torch.Size([4, 1, 8]) = (층 개수x2, 배치크기, 은닉상태 크기x2) : 정방향 기준으로는 마지막 시점에 해당되면서, 역방향 기준에서는 첫번째 시점에 해당되는 시점의 출력값을 층의 개수만큼 쌓아 올린 결과값 ??"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 10, 8])\n",
            "torch.Size([1, 1, 8])\n",
            "---------------------------\n",
            "torch.Size([1, 10, 8])\n",
            "torch.Size([2, 1, 8])\n",
            "---------------------------\n",
            "torch.Size([1, 10, 16])\n",
            "torch.Size([4, 1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maoj7psrDEpj"
      },
      "source": [
        "**10장 chap2 장단기 메모리 LSTM**\n",
        "\n",
        "바닐라 RNN : 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생 - 장기 의존성 문제\n",
        "\n",
        "Long Short-Term Memory (LSTM)\n",
        "\n",
        ": 은닉층의 메모리 셀에 입력 게이트, 삭제 게이트, 출력 게이트를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정함\n",
        "\n",
        "- 입력게이트 (i_t, g_t -> 셀 상태에서 기억할 양 정함) : 현재 시점의 입력을 얼마나 반영할 지 결정\n",
        "\n",
        "    => i_t: 입력&이전 은닉상태 -> 시그모이드\n",
        "\n",
        "    => g_t: 입력&이전 은닉상태 -> tanh\n",
        "- 삭제게이트 (f_t: 삭제과정 거친 후의 정보의 양) : 이전 시점의 입력을 얼마나 반영할 지를 의미\n",
        "\n",
        "    => f_t: 입력&이전 은닉상태 -> 시그모이드 \n",
        "\n",
        "- 셀 상태(장기상태) (C_t)\n",
        "\n",
        "    => C_t = f_t * C_t-1 + i_t * g_t ( *는 원소별 곱), i_t * g_t는 이번에 선택된 기억할 값\n",
        "\n",
        "- 출력게이트(o_t)와 은닉상태(단기상태) (h_t)\n",
        "\n",
        "    => o_t: 입력&이전 은닉상태 -> 시그모이드\n",
        "\n",
        "    => h_t = o_t * tanh(C_t) : 현재의 은닉상태 & 출력층으로 향함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHdzJ4DYDKzf"
      },
      "source": [
        "# nn.RNN(input_dim, hidden_size, batch_fisrt=True) # 이거를\n",
        "# nn.LSTM(input_dim, hidden_size, batch_fisrt=True) # 이렇게 바꿔주면 됨"
      ],
      "execution_count": 4,
      "outputs": []
    }
  ]
}