{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap11_Char RNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP6pHVDMexyIiRgDWRlahC/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaheeKo/Deep-learning-study/blob/main/CODES/Chap11_Char_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIV7eE41dYus"
      },
      "source": [
        "**Chap 11. 다대다 RNN을 이용한 텍스트 생성**\n",
        "\n",
        "다대다 RNN : 모든 시점의 입력에 대해서, 모든 시점에 대해서 출력\n",
        "\n",
        "=> 품사 태깅, 개체명 인식 등에서 사용됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxiyMJEvdvLW"
      },
      "source": [
        "1. 문자 단위 RNN (Char RNN): RNN의 입출력 단위가 문자 레벨(character level)\n",
        "\n",
        "  예제) 문자 시퀀스 apple 입력 -> pple! 출력하도록 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLSRDH50dPQY",
        "outputId": "410e8dc6-834c-40d9-f6d8-3ebb585fa3ed"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# 훈련 데이터 전처리\n",
        "  # 입력 데이터와 레이블 데이터에 대해서 문자 집합(voabulary)을 만들기, 중복을 제거\n",
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))\n",
        "\n",
        "  # 하이퍼파라미터 정의\n",
        "input_size = vocab_size # 입력: 원-핫 벡터 사용할 것이므로 크기는 문자 집합의 크기 \n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1\n",
        "\n",
        "  # 고유한 정수 부여\n",
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # enumerate: 자료형 입력 받아 인덱스 값 포함하는 객체로 리턴\n",
        "print('char_to_index: ', char_to_index)\n",
        "\n",
        "  # 정수로부터 문자를 얻을 수 있는 함수 구현 (결과를 문자 시퀀스로 받기 위해)\n",
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print('index_to_char: ', index_to_char)\n",
        "\n",
        "  # 데이터의 각 문자들을 정수로 맵핑\n",
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print('x_data: ', x_data) # a, p, p, l, e에 해당\n",
        "print('y_data: ', y_data) # p, p, l, e, !에 해당\n",
        "\n",
        "  # 배치 차원 추가 <= nn.RNN()이 기본적으로 3차원 텐서를 입력 받기 때문\n",
        "    # 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있음.\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print('배치 차원 추가한 x_data: ', x_data)\n",
        "print('배치 차원 추가한 y_data: ', y_data)\n",
        "\n",
        "  # 입력 시퀀스의 각 문자들을 원-핫 벡터로 변경\n",
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print('x_one_hot: ', x_one_hot)\n",
        "\n",
        "  #입력 데이터와 레이블 데이터를 텐서로 바꿔줌\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 5\n",
            "char_to_index:  {'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n",
            "index_to_char:  {0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n",
            "x_data:  [1, 4, 4, 3, 2]\n",
            "y_data:  [4, 4, 3, 2, 0]\n",
            "배치 차원 추가한 x_data:  [[1, 4, 4, 3, 2]]\n",
            "배치 차원 추가한 y_data:  [[4, 4, 3, 2, 0]]\n",
            "x_one_hot:  [array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n",
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0ip1ZTpgjZ_",
        "outputId": "c98a21b1-a1fe-4f00-ca86-1c4426e011a6"
      },
      "source": [
        "# 모델 구현하기 \n",
        "\n",
        "  # 클래스로 모델 정의\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
        "\n",
        "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "  \n",
        "net = Net(input_size, hidden_size, output_size)\n",
        "\n",
        "outputs = net(X)\n",
        "print('outputs.shape: ', outputs.shape) # 3차원 텐서: (배치차원, 시점, 출력의 크기)\n",
        "      # 정확도 측정 시에는 모두 펼쳐서 계산: view 사용 -> 배치 차원과 시점 차원을 하나로\n",
        "print('2차원 텐서로 변환: ', outputs.view(-1, input_size).shape) \n",
        "print('Y.shape: ', Y.shape) \n",
        "print('2차원 텐서로 변환: ', Y.view(-1).shape)\n",
        "print()\n",
        "  # 옵티마이저, 손실 함수 정의\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
        "\n",
        "  # 100번의 에포크 학습\n",
        "for i in range(100):\n",
        "    optimizer.zero_grad() # Pytorch에서는 gradients값들을 추후에 backward를 해줄때 계속 더해주기 때문에 \n",
        "                          # backpropagation을 하기전에 gradients를 zero로 만들어주고 시작\n",
        "                          # 한 번 학습할 때마다 0으로 초기화\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() # 기울기 계산\n",
        "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
        "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outputs.shape:  torch.Size([1, 5, 5])\n",
            "2차원 텐서로 변환:  torch.Size([5, 5])\n",
            "Y.shape:  torch.Size([1, 5])\n",
            "2차원 텐서로 변환:  torch.Size([5])\n",
            "\n",
            "0 loss:  1.5979819297790527 prediction:  [[4 4 4 3 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppplp\n",
            "1 loss:  1.3819283246994019 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "2 loss:  1.268026351928711 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "3 loss:  1.15736722946167 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "4 loss:  1.0127224922180176 prediction:  [[4 4 2 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppee!\n",
            "5 loss:  0.8821309208869934 prediction:  [[4 4 2 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppee!\n",
            "6 loss:  0.7519644498825073 prediction:  [[4 4 3 3 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppll!\n",
            "7 loss:  0.6107199788093567 prediction:  [[4 4 3 3 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppll!\n",
            "8 loss:  0.51166170835495 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.38991397619247437 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.30904340744018555 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.24005651473999023 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.16768193244934082 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.1113307848572731 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.07461372017860413 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.05039427801966667 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.03393954038619995 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.02276991493999958 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.015365886501967907 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.01067273411899805 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.007834416814148426 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.006121987942606211 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.005032653920352459 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.004282359499484301 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.003723402973264456 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.0032804813235998154 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.0029147486202418804 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.002605259185656905 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.0023401493672281504 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0021116379648447037 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.0019143292447552085 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.0017438421491533518 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.0015965026104822755 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.0014692997792735696 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.0013591949827969074 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.001263766665942967 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.001180852996185422 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.0011084098368883133 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.0010448206448927522 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.0009888489730656147 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0009390672785229981 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0008946190355345607 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0008545999298803508 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.000818343716673553 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0007853025454096496 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0007550958544015884 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0007272709044627845 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0007015658775344491 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0006776473019272089 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0006554437568411231 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0006346931913867593 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0006153479916974902 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.0005971937207505107 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0005801828810945153 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0005641962634399533 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.000549210119061172 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.0005350576248019934 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0005217388388700783 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.0005091822822578251 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.0004972927272319794 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0004861654597334564 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0004755383706651628 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.00046550686238333583 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.00045597561984322965 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.00044694458483718336 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0004383662308100611 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0004301690496504307 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0004224245494697243 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.00041501346277073026 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.00040784067823551595 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.00040109670953825116 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.00039461487904191017 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0003884188481606543 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.00038248495548032224 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.00037676552892662585 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0003712843172252178 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.000366017542546615 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.00036091756192035973 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0003560796321835369 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0003514085547067225 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.00034688040614128113 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.00034251908073201776 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.000338348385412246 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0003343445132486522 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0003304597339592874 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0003267417778261006 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.00032311916584149003 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.0003196871548425406 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.0003163981600664556 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0003131567791569978 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.00031015375861898065 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.00030717451591044664 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0003042906173504889 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.00030157354194670916 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.00029892800375819206 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.000296425394481048 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0002939942933153361 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.00029168237233534455 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.0002894896315410733 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.0002872968907468021 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5aVfA_Bj_9f"
      },
      "source": [
        "2. 더 많은 데이터 문자 단위 RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYRbO5MYkI65",
        "outputId": "506d87e5-4542-42ab-db3e-d2117a063329"
      },
      "source": [
        "# 훈련 데이터 전처리\n",
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "\n",
        "# 문자 집합 생성, 고유 정수 부여\n",
        "char_set = list(set(sentence))\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} \n",
        "print(\"문자 집합: \", char_dic)\n",
        "dic_size = len(char_dic) # 매 시점마다 들어갈 입력의 크기이기도 함\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size # 다른 값도 무방\n",
        "sequence_length = 10  # 임의 숫자 지정, 앞서 만든 샘플을 10개 단위로 끊어서 샘플로 만들 것임\n",
        "learning_rate = 0.1 # 학습률\n",
        "\n",
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence)-sequence_length):\n",
        "  x_str = sentence[i: i + sequence_length]\n",
        "  y_str = sentence[i+1: i + sequence_length + 1]\n",
        "  #print(i, x_str, ' -> ', y_str)\n",
        "\n",
        "  x_data.append([char_dic[c] for c in x_str]) # # x_str to index\n",
        "  y_data.append([char_dic[c] for c in y_str])  # y_str to index\n",
        "\n",
        "print('첫 번째 샘플의 입력 데이터: ', x_data[0])\n",
        "print('첫 번째 샘플의 레이블 데이터: ', y_data[0]) # 한 칸씩 쉬프트 된 시퀀스 출력\n",
        "\n",
        "# 입력 시퀀스에 대해 원-핫 인코딩 수행, 입력 및 레이블 데이터 텐서로 변환\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))\n",
        "\n",
        "print('원-핫 인코딩 된 결과: ', X[0])\n",
        "print('레이블 데이터의 첫 번째 샘플: ', Y[0]) # f you want에 해당\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합:  {'u': 0, 'a': 1, 't': 2, 's': 3, 'i': 4, 'l': 5, '.': 6, 'f': 7, 'd': 8, ',': 9, 'y': 10, 'e': 11, 'o': 12, 'n': 13, 'p': 14, 'k': 15, 'g': 16, \"'\": 17, 'h': 18, 'b': 19, 'm': 20, 'c': 21, ' ': 22, 'r': 23, 'w': 24}\n",
            "문자 집합의 크기 : 25\n",
            "첫 번째 샘플의 입력 데이터:  [4, 7, 22, 10, 12, 0, 22, 24, 1, 13]\n",
            "첫 번째 샘플의 레이블 데이터:  [7, 22, 10, 12, 0, 22, 24, 1, 13, 2]\n",
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n",
            "원-핫 인코딩 된 결과:  tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 1.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n",
            "레이블 데이터의 첫 번째 샘플:  tensor([ 7, 22, 10, 12,  0, 22, 24,  1, 13,  2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1EearLpm8i4",
        "outputId": "ead178d8-d4d7-46c0-b7f2-eb3e46a7d089"
      },
      "source": [
        "# 모델 구현하기\n",
        " # 은닉층 2개 쌓음\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True) # num_layers 은닉층의 수\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개\n",
        "\n",
        " # 비용 함수와 옵티마이저 선언\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
        "\n",
        "outputs = net(X)\n",
        "print('출력의 크기: ', outputs.shape) # 3차원 텐서 (배치차원, 시점, 출력의 크기)\n",
        "print('2차원 텐서로 변환: ', outputs.view(-1, dic_size).shape) \n",
        "print('레이블 데이터의 크기: ', Y.shape) \n",
        "print('2차원 텐서로 변환: ', Y.view(-1).shape)\n",
        "print()\n",
        "\n",
        " # 손실 함수와 옵티마이저 정의\n",
        "for i in range(100):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(X) # 매 에포크마다 모델의 입력으로 사용\n",
        "\n",
        "  loss = criterion(outputs.view(-1,  dic_size), Y.view(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  results = outputs.argmax(dim=2) # results의 텐서 크기는 (170, 10)\n",
        "  predict_str = \"\"\n",
        "\n",
        "  for j, result in enumerate(results):\n",
        "    if j==0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "      predict_str += ''.join([char_set[t] for t in result])\n",
        "    else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "      predict_str += char_set[result[-1]]\n",
        "      \n",
        "  print(predict_str) # 처음에는 이상한 예측, 마지막 에포크에서는 꽤 정확한 문자 생성\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "출력의 크기:  torch.Size([170, 10, 25])\n",
            "2차원 텐서로 변환:  torch.Size([1700, 25])\n",
            "레이블 데이터의 크기:  torch.Size([170, 10])\n",
            "2차원 텐서로 변환:  torch.Size([1700])\n",
            "\n",
            "ggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggggg\n",
            "                                                                                                                                                                                   \n",
            "    t t t t  t t t t  t t t t t t   t      t t t     t t t t tt t t t           t t t t t t t t t t   t t t t t t t t t t t    t t t t t    t t t t t t t t t t      t t t t t t t \n",
            "t ghtryhdrrirrrirriririrrraririrrrirrrririririhhiriiiirnirrrrhrrirhririririrrrarnrirrririrrrirorirrrrrrrrrrhroiririiarhoirhrrioorirhrrrrirrrirrriraiirrirhhrirriiriiidrirohrrirriir\n",
            "toooaoooooooooooooooooooo ooooo oooooooooooooo oo ooo ooooooooooo o ooooo ooooooooo ooooooooooooooooooooooooooooooo o ooooooooooooooooooo  o  ooooooo ooooooooooooeoooooooooooooooo\n",
            "t toe aoilt l l lnl  iantltn ntnilantli nl l nltlllll l ll  nll  lnltllnll n lanantltntlnl ltln tlnl  antnt tltltntn nnlnl    ntl n nn lnl   ltlnn n l lalto l  ll llnil  n ntoilan\n",
            "t t e t  t t     tt t     t     t t  t         t   t      t t   t     t   t   t     t               t   t   t     t t      t t   t  t t   t t         t t     t   t     tt t   t   \n",
            "t   t t          tt t     t       t                                             t   t                                            t        t t         t t     t                    \n",
            "t          t        t             t      t             t        t           t t                                                  t        t       t                              t \n",
            "t          t   t tt a t           t        t           t     tt t     t     t t t   t   t             t             t            t        a t     t   t       t         t  t     t \n",
            "t t   t  a     t d  a t     t   t t              t   t t  t  th t    t      t   t t t   t t   e t     t     t t t t t      t   t t    t     t     t   t t     t   t  t     t   t   \n",
            "t t  ot  a   tot t  aot to  t eot t  o  t to totot  ot to t  to to o to t oot t totot   t to    ta  o t totot t t aot  ot  o  tt a eo t too t t t t e tot     t   to t  ot t e toto\n",
            "t t  et  a t tp  ao a t toa t apt t ao  to t tetoaoaot e  t tt et  o te   tnt t totot   t apheo     o t t tea a tot t t t  e   tet a  t tto t t t t a tet a   t a to t epe t a tot \n",
            "l t  to  t t t  e t e t e t t tpt t et tee t eet t en  e  t tpeeto e te   tnt t tptoe   t tp e  e e t t t tne e t e t e tn e   tel ep t tpe t t t t e t t t   t   e  t tpt t t t t \n",
            "l t  to  t t et e t t t tp  t ept t  t tpete  o ntoeee e  t epoet  t tt   tnt t e the   t tp e  t   t t t t   t t t t      t en e  eo t  te t   e e e tet t   t  ee  t tp  t e t t \n",
            "t to  te t thtc tlt t t epe t epe e  t ttete n t t e e e  t ncoeto t tln  e t t e t e   t tt e  t   t t t t e e t e t t e  t e tet e  t  te t t t e e t t t   t  te  t  t  t e t t \n",
            "t tte tt t thtm tlt t t ele d emt t  t ttete ece t   t e  to toeto t tl   t d t tmt e s t tt eo to  t t thtme t toe t e e  toe tetoel tostt   t t t e t t t s t  te  to ce t e t  l\n",
            "t to  tt t tod' tlt d d e'e d e'tot  t ttete ece toe s t  to tuelt t tte  t d t d't t s t tt eo tos t d dhdo  t tut d e e  t  ctetoeo to te t t d t e d t t s toutl  t  te d e t d \n",
            "t to  tt t toet ele d t e'e d e'tot  t toete  le toe s e  to tuel nt tt   t d t e't t s   tt eo to  t t t to  t tte d s e sto cset eo to 'tnt t e t e d toe s ttue t t  t  toe t e \n",
            "t ao  ttnt doet ele d d ele d e't d  m toepe  le toe s e  to thelendott   t d a  't a s s  t eo to  e t   do  e tue d e eo to lsetheo to tent toe aoe d t eos tl ee  t  te toe d e \n",
            "t aoe tt t t etuele d d e'e d e't a  e  oepe ele  o  t en to to tend tt   ene d e't d s e  the  to  e d d dl  e tue d e eo to  hethel to te dotoe a e d d e s t  ee  t  te toent e \n",
            "t ao  tand toetatle t d e'e d e't t  l to pe doe to  s e  to 'h lent to t t d t e't t s g to e  to  s t dotl  e tut t e eo to lhethel to 'entotoe doe dnd e   tpuee  t  le toe t d \n",
            "t ao ttantot  batle t d epe d e't t  l to penene to    e  to bh le t tont t d toeot t   g to tm to  s t t to  e tut t e e  to lhethel to tong t   toe d t e   gpue   t ao  toe t d \n",
            "t do ttontoto butle t d e'e d e't t  e  o pe dne  o  g e  to pollo totou  tnd ahe't a   g dohem to  s d d tof e dutod t eo to lh thel to tong t d the dnd e   gpuels to pf toe d d \n",
            "g do ttont t dcuold d d epe d e't d  e  o ce dne do  t e  to chlle t ton  dnd ahe't a s g  them to  s d d do  e dut d g e  to ch chem to po g to  the dnd e s smuils t  cf toe d d \n",
            "gmto ttont th cutrd t d e'e doe't to e  o pe tne   d   e  to colle t tome t d toe't a s g  them tos s tnd to  e dut datoer to chethem to cong to  toe dnd e sitmui s to pf toe d d \n",
            "gmto ttont to cutrd t d e', doe't ao l uo te dne to  t e  to colle toto d tnd toe't a sis  them tos s tnd to  e dut datoer to cheohem to cong to utoe d d e sitmuels th tf toe d ds\n",
            "gmtl  tond to cutrd t d e't doe't a  l uo pe dne to  t e  to conle t to d tnd toe't a sig  them tos s tnd to  e dut dutoer to chephem to cong toeutoe dnd e sitmuens iy tf toe d s \n",
            "gmtl ktond to tutld t d e', toe't a  m uo ceodlento  t e  to conle t to d tnd toe't a  ig  them tos s tnd to  e dut dutoer to cheahem to long toe toerdndoersitm ens ty af toe t gs\n",
            "mmao  tond to tueld a d e', doe't a  m umep,otlento  t e  to lolle a tond tnd toe't a  ig  them to  s tnd ton e tut dutoe  to chemhem to long tor the dng eos imoens ty mf toe a ts\n",
            "mmao  tond to tueld a d ep, don't a  m um p,oplecto  the  to lollech tond tnd aoe't a sig  them tos s tnd aor e tut rutoe  to chethem to long tor the dng eos im e s ty mf the a gs\n",
            "lmaor tond to tueld a d ep, don't a  m up p,ople to ethe  to lollect toodoand aon't a sig  them tos s tnd dor e tut rutoe  to ch them to long tor the dnd eos ip e s ty mf the s ps\n",
            "lmao  tont to cuele d d ep, don't a  m up peodle to ethe  th collect doododnd don't a sign them tos s tnd dorke but rutoer to ch them to long tor the dnd e s im insity mf therd ts\n",
            "lhao ttont to cueld d d ep, don't d  m up people to kthe  ty lollect tood dnd don't a sign them tos s tnd dorke but r ther th ch them to long tor the snd e s im ensity mf therd tn\n",
            "lmdo  tont to bueld d d ep, don't d  m up peodle to  the  ty lollect tood dnd don't d sign them tos s tnd dorke but ruther th ch them to long tor the snd ess immens ty of the s tn\n",
            "lmaor tont ao cueld d s ep, don't a  m up peodle tog the  th lollect tord dnd don't a sign them tos s and dork, but r ther th ch them to long tor the snd ess immens ty of the s tn\n",
            "lmaor tontoao cueld d shep, don't a  m up peodlertog the  th lollect aord dnd don't a sign them tos s and aork, but rather th ch them to long tor the snd ess immens ty of the s tn\n",
            "lmao  tont to bueld d shep, don't a  m up peorlertog the  to bollect aord dnd aon't assign them tos s and aork, but rather to ch them to long tor the snd ess immens ty of the s tn\n",
            "lmao  tont to build a shep, don't a um up pemplertogethe  to lollect ao d and aon't assign them tos s and aork, but rather to ch them to long tor the snd ess immens ty of the s gn\n",
            "lmao ttont to build a ahep, don't a um up pemple togethe  th bollect aood and aon't assign them tos s and aork, but rather to ch them to bong tor the sndoessiimmensity of the shtn\n",
            "lmao ttont to cuild a ship, don't a um up people togethe  to collect aood and aon't assign them tos s and aork, but rather to ch them to long tor the sndlessiimmensity of the shpn\n",
            "lmao  tont to cuild a ship, don't a um up people togethe  to collect wood and don't assign them tos s and aork, but rathe  to ch them to long tor the sndlessiimmensity of the shps\n",
            "fmao  tont to cuild a ship, don't a um up people togethe  to collect wood and don't assign them tosks and aork, but rather to ch them to long tor the sndlessiimmensity of the shps\n",
            "fmdor tont to cuild a ship, don't a um up people together to collect wood and don't assign them tosks and aork, but rather to ch them to long tor the sndlessiimmensity of the seps\n",
            "tmdou tont to build a ship, don't arum up people togethe  to collect dood dnd don't assign them tosks and dork, but rather to ch them to long tor the sndless immensity of the seps\n",
            "tmdou tont to build a ship, don't arum up people together to bollect wood dnd don't assign them tosks and dork, but rather toach them to long tor the sndless immensity of the sepn\n",
            "tmdou tant to build a ship, don't arum up people together to bollect wood and don't assign them tosks and aork, but rather toach them to long tor the sndless immensity of the shps\n",
            "t dou want to build a ship, don't arum up people together to bollect wood dnd don't assign them tosks and dork, but rather toach them to long tor the sndless immensity of the seps\n",
            "t sou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and dork, but rather toach them to long tor the sndless immensity of the se s\n",
            "t sou want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and dork, but rather toach them to long tor the sndless immensity of the se c\n",
            "t sou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and aork, but rather toach them to long tor the sndless immensity of the se c\n",
            "t sou want to build a ship, don't doum up people together to collect wood and don't assign them tosks and aork, but rather toach them to long tor the sndless immensity of the seac\n",
            "t sou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and aork, but rather toach them to long tor the sndless immensity of the seac\n",
            "t sou want to build a ship, don't doum up people together to collect wood and don't assign them tosks and aork, but rather toach them to long tor the sndless immensity of the seas\n",
            "t sou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather toach them to long for the sndless immensity of the seas\n",
            "t sou want to build a ship, don't arum up people together te bollect wood and don't assign them tosks and aork, but rather teach them to long for the sndless immensity of the seac\n",
            "t sou want to cuild a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather teach them to long for the sndless immensity of the seas\n",
            "t sou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather teach them to long for the sndless immensity of the seas\n",
            "t sou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and aork, but rather toach them to cong for the sndless immensity of the seac\n",
            "t sou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and aork, but rather toach them to long for the sndless immensity of the seac\n",
            "t sou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather teach them to long for the sndless immensity of the seac\n",
            "t sou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and dork, but rather teach them to long for the sndless immensity of the seac\n",
            "t sou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and dork, but rather teach them ta long for the sndless immensity of the seas\n",
            "l sou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the sea.\n",
            "l sou tant to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the eeac\n",
            "l sou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather toach them ta long for the endless immensity of the eea.\n",
            "t sou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather toach them to long for the endless immensity of the eea.\n",
            "p sou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the eea.\n",
            "p sou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the seac\n",
            "p sou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p sou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p sou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather teach them to long for the endless immensity of the sea.\n",
            "g sou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the sea.\n",
            "g sou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and dork, but rather teach them to long for the endless immensity of the sea.\n",
            "g sou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and dork, but rather teach them to long for the endless immensity of the sea.\n",
            "g sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g sou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p bou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p bou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "l sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "l sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "g sou want to build a ship, don't drum up people together to lollect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "g sou want to build a ship, don't drum up people together te collect wood and don't assign them tasks and work, but rather teach them to cong for the endless immensity of the sea.\n",
            "g bou want to build a ship, don't drum up people together te collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "g bou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "m sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them ta long for the endless immensity of the sea.\n",
            "m sou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the eea.\n",
            "m sou want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the eea.\n",
            "m sou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
            "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "g sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "g sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p sou want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfhLRBEXp9vw"
      },
      "source": [
        "3. 단어 단위 RNN - 임베딩 사용\n",
        "\n",
        "예제)  'Repeat is the best medicine for' 입력 \n",
        "          -> 'is the best medicine for memory' 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcSF4I4nqA7L",
        "outputId": "f06942ec-61df-4a7d-c93e-91a549f89740"
      },
      "source": [
        "# 훈련 데이터 전처리\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "sentence = \"Repeat is the best medicine for memory\".split()\n",
        "\n",
        "vocab = list(set(sentence))\n",
        "print('단어 집합: ', vocab)\n",
        "\n",
        "word2index = {tkn: i for i, tkn in enumerate(vocab,1)} # 고유한 정수 부여\n",
        "word2index['<unk>'] = 0\n",
        "print('word2index: ', word2index) # unk 토큰 추가된, 우리가 사용할 vocabulary\n",
        "\n",
        " # 수치화된 데이터를 단어로 바꾸기 위한 사전\n",
        "index2word = {v: k for k, v in word2index.items()}\n",
        "print('index2word: ', index2word)\n",
        "\n",
        "def build_data(sentence, word2index):\n",
        "    encoded = [word2index[token] for token in sentence] # 각 문자를 정수로 변환. \n",
        "    input_seq, label_seq = encoded[:-1], encoded[1:] # 입력 시퀀스와 레이블 시퀀스를 분리\n",
        "    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    return input_seq, label_seq\n",
        "\n",
        "X, Y = build_data(sentence, word2index) # 입력 데이터, 레이블 데이터\n",
        "print('입력 데이터: ', X) # Repeat is the best medicine for\n",
        "print('레이블 데이터: ', Y) # Repeat is the best medicine for\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어 집합:  ['medicine', 'the', 'memory', 'is', 'Repeat', 'best', 'for']\n",
            "word2index:  {'medicine': 1, 'the': 2, 'memory': 3, 'is': 4, 'Repeat': 5, 'best': 6, 'for': 7, '<unk>': 0}\n",
            "index2word:  {1: 'medicine', 2: 'the', 3: 'memory', 4: 'is', 5: 'Repeat', 6: 'best', 7: 'for', 0: '<unk>'}\n",
            "입력 데이터:  tensor([[5, 4, 2, 6, 1, 7]])\n",
            "레이블 데이터:  tensor([[4, 2, 6, 1, 7, 3]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSyMyux1rU7g",
        "outputId": "601aa92e-f24f-457c-9e62-f46b2afde37d"
      },
      "source": [
        "# 모델 구현 **임베딩 층을 추가**\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n",
        "                                            embedding_dim=input_size)\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\n",
        "                                batch_first=batch_first)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # 1. 임베딩 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        output = self.embedding_layer(x)\n",
        "        # 2. RNN 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\n",
        "        output, hidden = self.rnn_layer(output)\n",
        "        # 3. 최종 출력층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n",
        "        output = self.linear(output)\n",
        "        # 4. view를 통해서 배치 차원 제거\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n",
        "        return output.view(-1, output.size(2))\n",
        "\n",
        "  # 하이퍼 파라미터\n",
        "vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\n",
        "input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
        "hidden_size = 20  # RNN의 은닉층 크기\n",
        "\n",
        "  # 모델 생성\n",
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
        "  # 손실함수 정의\n",
        "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨.\n",
        "  # 옵티마이저 정의\n",
        "optimizer = optim.Adam(params=model.parameters())\n",
        "\n",
        "  # 임의로 예측해보기. 가중치는 전부 랜덤 초기화 된 상태이다.\n",
        "output = model(X)\n",
        "print('임의 예측, 출력 확인용: \\n', output)\n",
        "print('예측값의 크기: ', output.shape) # (시퀀스의 길이, 은닉층의 크기)\n",
        "\n",
        "  # 수치화된 데이터를 단어로 전환하는 함수\n",
        "decode = lambda y: [index2word.get(x) for x in y]\n",
        "\n",
        " # 학습하기\n",
        "for step in range(201):\n",
        "    optimizer.zero_grad()    # 경사 초기화\n",
        "    output = model(X)    # 순방향 전파\n",
        "    loss = loss_function(output, Y.view(-1))    # 손실값 계산\n",
        "    loss.backward()     # 역방향 전파\n",
        "    optimizer.step()    # 매개변수 업데이트\n",
        "    # 기록\n",
        "    if step % 40 == 0:\n",
        "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
        "        pred = output.softmax(-1).argmax(-1).tolist()\n",
        "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
        "        print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "임의 예측, 출력 확인용: \n",
            " tensor([[-0.4572, -0.2625, -0.2009, -0.0724, -0.3146, -0.1375,  0.0638, -0.0365],\n",
            "        [-0.2208, -0.3214, -0.0030,  0.0147,  0.0791,  0.0096,  0.1051, -0.0884],\n",
            "        [ 0.0016,  0.0666,  0.1854, -0.1536,  0.2216,  0.2227,  0.1517, -0.1729],\n",
            "        [-0.1251, -0.0313, -0.0925,  0.0954, -0.1182, -0.0539,  0.1845, -0.3352],\n",
            "        [ 0.1786, -0.1765,  0.4881,  0.0933,  0.7525,  0.1099,  0.1881, -0.0014],\n",
            "        [-0.1199,  0.1940,  0.2107,  0.0678,  0.3825,  0.1778,  0.1012, -0.1582]],\n",
            "       grad_fn=<ViewBackward>)\n",
            "예측값의 크기:  torch.Size([6, 8])\n",
            "[01/201] 2.1316 \n",
            "Repeat best best Repeat best is is\n",
            "\n",
            "[41/201] 1.5245 \n",
            "Repeat memory the best medicine for memory\n",
            "\n",
            "[81/201] 0.8926 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/201] 0.4468 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/201] 0.2281 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[201/201] 0.1291 \n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ]
        }
      ]
    }
  ]
}