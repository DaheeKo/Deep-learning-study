{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap 09_Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOGIdvAG097PYyOgmhW9pLm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DaheeKo/Deep-learning-study/blob/main/CODES/Chap_09_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjQJaU_OLEFz"
      },
      "source": [
        "**9장 chap7 파이토치의 nn.Embedding()**\n",
        "\n",
        "임베딩 층을 만들어 훈련데이터로부터 임베딩 벡터 학습 <- nn.Embedding() 사용해 구현\n",
        "\n",
        "\n",
        "입력 시퀀스의 각 단어들 정수 인코딩 -> 임베딩 층의 입력으로 사용할 수 있음\n",
        "\n",
        "=>  단어 -> 고유한 정수 인덱스 -> 임베딩 층 통과 -> 밀집벡터(임베딩 벡터)로 맵핑\n",
        "\n",
        "* 임베딩 층 - 단어에게 부여된 정수를 인덱스로 하는 룩업 테이블. \n",
        "  \n",
        "  단어의 정수 인덱스를 임베딩 층의 입력으로 사용 -> 룩업 테이블 -> 단어의 인덱스에 해당하는 행을 그 단어의 임베딩 벡터로 리턴 ->  임베딩 벡터가 모델의 입력이 되어 역전파 과정에서 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-23T-3sicqW",
        "outputId": "fcddbef1-aa78-4e66-de8f-06e44f35915e"
      },
      "source": [
        "import torch\n",
        "\n",
        "# 임베딩 층 = 룩업테이블인 거 이해하기\n",
        "\n",
        "train_data = 'you need to know how to code'\n",
        "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성\n",
        "vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 맵핑, 0과1은 비워줘야 하니까 i+2부터\n",
        "vocab['<unk>'] = 0 \n",
        "vocab['<pad>'] = 1\n",
        "print(vocab)\n",
        "\n",
        "# 단어 집합의 크기만큼의 행을 가지는 테이블 생성\n",
        "embedding_table = torch.FloatTensor([[ 0.0,  0.0,  0.0],# unk\n",
        "                               [ 0.0,  0.0,  0.0], # pad\n",
        "                               [ 0.2,  0.9,  0.3], # to\n",
        "                               [ 0.1,  0.5,  0.7], #you\n",
        "                               [ 0.2,  0.1,  0.8], #know\n",
        "                               [ 0.4,  0.1,  0.1], # need\n",
        "                               [ 0.1,  0.8,  0.9], # how\n",
        "                               [ 0.6,  0.1,  0.1]]) # code\n",
        "\n",
        "sample = 'you need to run'.split() # 임의의 샘플\n",
        "idxes=[]\n",
        "# 각 단어 정수로 변환\n",
        "for word in sample:\n",
        "  try:\n",
        "    idxes.append(vocab[word]) # 단어집합에 부여한 정수에서, word에 해당하는 값을 idxes에 넣음\n",
        "  except KeyError: \n",
        "    idxes.append(vocab['<unk>']) # 단어 집합에 없는 단어일 경우 <unk>로 대체\n",
        "idxes = torch.LongTensor(idxes)\n",
        "print(idxes)\n",
        "\n",
        "# 룩업 테이블\n",
        "lookup_result = embedding_table[idxes,:] # 각 정수를 인덱스로 임베딩 테이블에서 행을 가져온다\n",
        "print(lookup_result) # 1행: you 2행: need 3행:to 4행: run(embedding_table에 없어서 0으로 나옴)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'know': 2, 'to': 3, 'you': 4, 'code': 5, 'need': 6, 'how': 7, '<unk>': 0, '<pad>': 1}\n",
            "tensor([4, 6, 3, 0])\n",
            "tensor([[0.2000, 0.1000, 0.8000],\n",
            "        [0.1000, 0.8000, 0.9000],\n",
            "        [0.1000, 0.5000, 0.7000],\n",
            "        [0.0000, 0.0000, 0.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huMAmaUyowzA",
        "outputId": "0c9d5bd5-5b85-48ea-aa56-416e65dff633"
      },
      "source": [
        "# nn.Embedding() 사용하기\n",
        "\n",
        " # 전처리\n",
        "train_data = 'you need to know how to code'\n",
        "word_set = set(train_data.split()) # 중복을 제거한 단어들의 집합인 단어 집합 생성\n",
        "vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}  # 단어 집합의 각 단어에 고유한 정수 맵핑\n",
        "vocab['<unk>'] = 0 \n",
        "vocab['<pad>'] = 1\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "# 임베딩 테이블 만들기\n",
        "embedding_layer = nn.Embedding(num_embeddings = len(vocab), # 임베딩을 할 단어들의 개수 == 단어 집합의 크기\n",
        "                               embedding_dim = 3, # 임베딩 할 벡터의 차원. 사용자가 정해주는 하이퍼파라미터\n",
        "                               padding_idx = 1) # 선택적으로 사용하는 인자, 패딩을 위한 토큰의 인덱스\n",
        "\n",
        "print(embedding_layer.weight) # 단어 집합의 크기의 행을 가지는 임베딩 테이블"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 1.1968, -0.6183,  0.2987],\n",
            "        [ 0.0000,  0.0000,  0.0000],\n",
            "        [ 0.5569, -0.5075,  1.2371],\n",
            "        [-1.9764, -0.0103,  1.9710],\n",
            "        [ 1.9768,  1.5862, -0.4923],\n",
            "        [ 1.4437,  0.7570, -1.0409],\n",
            "        [-1.2703, -0.3534,  1.6235],\n",
            "        [-1.4223, -0.0204, -0.6342]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNu_yjLZOL-r"
      },
      "source": [
        "**9장 Chap8 사전 훈련된 워드 임베딩**\n",
        "\n",
        "훈련 데이터가 적다면 파이토치의 nn.Embedding()으로 해당 문제에 충분히 특화된 임베딩 벡터를 만들어내는 것이 어려움 \n",
        "\n",
        "=> 보다 일반적이고 보다 많은 훈련 데이터로 이미 Word2Vec이나 GloVe 등으로 학습되어져 있는 임베딩 벡터들을 사용할 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zguED8RUqEtv",
        "outputId": "a46e66a8-0b67-494c-ddaf-be4d631f79fb"
      },
      "source": [
        "# 훈련 데이터\n",
        "from torchtext.legacy import data, datasets\n",
        "\n",
        "# 필드 정의 \n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.Field(sequential=False, batch_first=True)\n",
        "\n",
        "#  torchtext.datasets => IMDB 데이터셋 다운로드, 학습 데이터셋과 테스트 데이터셋으로 나누기\n",
        "trainset, testset = datasets.IMDB.splits(TEXT, LABEL) # 훈련 데이터: 25,000개\n",
        "\n",
        "\n",
        "# 1. 직접 훈련시킨 사전 훈련된 워드 임베딩을 사용하는 방법\n",
        " # 생략\n",
        "\n",
        "# 2. 토치텍스트에서 제공하는 사전 훈련된 워드 임베딩을 사용하는 방법\n",
        "from torchtext.vocab import GloVe\n",
        " # build_vocab을 통해 토치텍스트가 제공하는 사전 훈련된 임베딩 벡터를 다운\n",
        "TEXT.build_vocab(trainset, vectors=GloVe(name='6B', dim=300), max_size=10000, min_freq=10)\n",
        "  #  max_size: 단어 집합의 크기 제한 / min_freq: 최소 등장 빈도수\n",
        "LABEL.build_vocab(trainset)\n",
        "\n",
        " # 확인해보기\n",
        "print(TEXT.vocab.stoi) \n",
        "print('임베딩 벡터의 개수와 차원 : {} '.format(TEXT.vocab.vectors.shape))\n",
        "print(TEXT.vocab.vectors[0]) # <unk>의 임베딩 벡터 값 : 0으로 초기화된 상태\n",
        "print(TEXT.vocab.vectors[1]) # <pad>의 임베딩 벡터 값 : 0으로 초기화된 상태\n",
        "print(TEXT.vocab.vectors[10]) # this의 임베딩 벡터 값 : 이번에 다운로드 받은 단어 this의 사전 훈련된 임베딩 벡터 값\n",
        "print(TEXT.vocab.vectors[9999]) # 'seeing'의 임베딩 벡터 값: 0으로 초기화된 상태 <= 사전훈련된 임베딩 벡터의 단어장에 'seeing' 존재 x\n",
        "\n",
        " # **TEXT.vocab.vectors를 nn.Embedding()의 초기화 입력으로 사용\n",
        "embedding_layer = nn.Embedding.from_pretrained(TEXT.vocab.vectors, freeze=False)\n",
        "embedding_layer(torch.LongTensor([10])) # 단어 this의 임베딩 벡터값"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-2.0437e-01,  1.6431e-01,  4.1794e-02, -1.3708e-01, -2.9779e-01,\n",
              "          3.3440e-01, -6.9955e-02, -6.8036e-02,  1.0604e-01, -2.0337e+00,\n",
              "          1.7977e-01, -7.7403e-02, -1.9518e-01,  1.8324e-01,  3.0017e-02,\n",
              "         -5.4762e-02, -4.5725e-01, -2.4509e-02,  5.7387e-02, -3.4878e-01,\n",
              "          3.9696e-02,  4.4826e-01, -5.8462e-02,  4.1181e-01, -3.5411e-02,\n",
              "         -1.4722e-01,  1.0740e-01, -2.5896e-01, -1.1658e-01,  1.9822e-01,\n",
              "          3.2850e-01,  2.4177e-01, -5.7177e-01, -5.6442e-02, -9.6437e-01,\n",
              "          3.4482e-01,  5.4639e-02,  2.3828e-01, -1.9139e-01,  3.0899e-01,\n",
              "          2.8044e-01, -3.3814e-02, -2.5436e-01,  1.5373e-02,  1.6341e-01,\n",
              "          2.6352e-01,  1.5812e-01,  3.2044e-01, -2.3082e-01,  2.6050e-01,\n",
              "          2.0606e-01, -8.9769e-02, -1.0055e-01,  7.0378e-02, -2.7452e-02,\n",
              "          2.7959e-01, -9.5862e-02,  2.0574e-01,  2.9522e-01, -1.2285e-02,\n",
              "          1.1164e-01, -5.1373e-02,  4.6106e-01,  2.3014e-02, -3.7141e-01,\n",
              "         -2.4166e-01,  3.3773e-02,  3.6827e-02,  1.6918e-01, -1.0802e-01,\n",
              "         -1.0691e-01,  1.0219e-01,  1.0065e-01,  5.5907e-02, -2.7402e-01,\n",
              "         -1.3689e-01,  4.2095e-01, -2.4060e-02, -3.2099e-01,  3.2065e-01,\n",
              "         -1.6776e-01,  1.0170e-01,  7.4999e-02, -1.0486e-01,  3.7114e-01,\n",
              "          3.2972e-01, -3.3043e-01,  3.8343e-01,  2.4555e-01,  2.0386e-01,\n",
              "         -4.1919e-01,  1.1570e-01, -7.8632e-02, -4.3171e-01, -2.3550e-02,\n",
              "         -1.1618e-01, -2.5605e-01,  3.4693e-01,  2.0398e-01, -1.7857e-01,\n",
              "          1.7301e-01,  4.6408e-02, -1.0486e-01,  9.8706e-02, -3.2077e-02,\n",
              "         -3.0462e-01,  1.2826e-01,  6.7985e-02, -2.5993e-01,  3.8041e-01,\n",
              "          4.5252e-02, -9.1834e-02, -4.5206e-01, -1.2498e-01,  1.7515e-01,\n",
              "         -1.3551e-01, -2.0556e-03, -9.3906e-02, -2.8006e-02, -4.6975e-01,\n",
              "          8.4430e-03, -2.4092e-01,  1.6000e-01,  2.2063e-01,  3.6277e-01,\n",
              "         -6.7643e-02,  2.8755e-01,  1.2643e-01,  1.2202e-01,  1.0548e-01,\n",
              "          4.0249e-01,  2.9781e-01,  4.9507e-01, -1.1096e-01, -2.4472e-01,\n",
              "          1.8720e-01,  1.1156e-01,  1.5680e-02,  7.6296e-03,  1.4304e-01,\n",
              "         -2.9299e-01,  1.7912e-01,  1.1604e-02, -5.6776e-02, -5.0224e-01,\n",
              "         -4.7262e-01,  1.5790e-01,  3.1573e-01, -7.7839e-02,  3.5172e-01,\n",
              "          1.6097e-01, -2.2595e-01,  2.4629e-01, -3.8200e-02,  6.4918e-01,\n",
              "          5.9545e-02, -5.0641e-02,  1.9511e-01, -6.8791e-02, -1.5146e-01,\n",
              "          1.2101e-02, -4.5943e-01,  1.4300e-02, -8.7461e-02, -3.2711e-02,\n",
              "          2.4036e-01,  1.7293e-02,  1.1340e-01, -3.4248e-02,  5.0351e-02,\n",
              "          9.3530e-02, -6.4975e-02, -8.5025e-01, -1.3809e-01, -3.4919e-01,\n",
              "         -2.0540e-02, -3.7268e-01,  4.7773e-02,  4.7216e-02,  2.3236e-01,\n",
              "          1.3777e-01,  2.4962e-01,  1.3133e-01,  4.7732e-02,  4.4829e-02,\n",
              "         -1.3243e-01, -1.6702e-01,  2.1045e-01, -4.0940e-02,  3.1555e-01,\n",
              "         -5.1593e-01,  1.0297e-01,  2.9704e-01,  1.6769e-02, -2.1701e-02,\n",
              "          5.7481e-03,  6.0955e-02, -2.2314e-02,  1.6080e-01, -2.1614e-01,\n",
              "          1.0959e+00, -4.0587e-01, -1.4514e-01, -1.3610e-01,  1.1280e-01,\n",
              "          1.7697e-01, -6.5900e-02, -1.3467e-01, -5.1049e-02, -2.8790e-01,\n",
              "         -3.9537e-01,  7.9347e-02,  5.7817e-01, -1.2027e-02, -1.2462e-01,\n",
              "          4.0711e-02,  1.3596e-02,  2.0398e-01,  9.5604e-02,  6.8153e-03,\n",
              "          2.5994e-01, -1.0152e-01, -3.8128e-01,  4.2629e-01,  1.8734e-01,\n",
              "          7.3060e-03,  6.0062e-01,  2.1663e-01,  2.3836e-02,  1.2912e-02,\n",
              "         -3.0333e-01,  3.1381e-01, -2.6096e-02, -3.8907e-01,  5.5081e-02,\n",
              "         -5.0901e-02,  2.5939e-01, -2.6417e-01,  2.0716e-01,  2.2498e-01,\n",
              "          1.9117e-01,  1.2614e-01,  1.4713e-01,  1.0489e-01, -1.1291e+00,\n",
              "         -8.1722e-02,  1.2693e-01,  1.5365e-01, -8.2781e-02, -3.5168e-01,\n",
              "          1.7873e-01, -9.7911e-02, -2.5831e-01,  9.0010e-03,  3.9271e-01,\n",
              "          9.9305e-02,  2.0227e-02,  9.2149e-03,  3.3352e-01, -7.1636e-02,\n",
              "         -5.9093e-02,  9.9506e-02,  3.1135e-01,  3.1324e-01, -1.0208e-01,\n",
              "         -6.0717e-02, -4.3183e-02, -8.3102e-02,  5.3218e-01, -1.6997e-01,\n",
              "          1.1647e-01, -1.0793e-01, -3.3692e-02,  1.6272e-01,  2.0517e-01,\n",
              "          1.1426e-01, -2.0803e+00, -4.4386e-03,  8.7898e-01,  4.7096e-01,\n",
              "         -2.7657e-01, -1.9387e-01, -9.8869e-02, -1.1244e-01, -1.4206e-01,\n",
              "          9.0613e-02, -1.8396e-01,  3.6844e-02, -1.9090e-01,  8.6006e-02,\n",
              "          9.2397e-04, -4.1547e-01, -7.7672e-02,  5.0569e-01,  2.4725e-01,\n",
              "          2.4119e-01, -1.3455e-01, -3.4007e-01, -7.7146e-02, -8.4089e-02]],\n",
              "       grad_fn=<EmbeddingBackward>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_mVwoNl_Szi"
      },
      "source": [
        "QUIZ ) 모델에 대한 설명이 다음과 같을 때, 총 파라미터 개수를 구해보세요.\n",
        "\n",
        "1. Embedding을 사용하며, 단어 집합(Vocabulary)의 크기가 5,000이고 임베딩 벡터의 차원은 100입니다.\n",
        "\n",
        "2. 은닉층에서는 Simple RNN을 사용하며, 은닉 상태의 크기는 128입니다.\n",
        "\n",
        "3. 훈련에 사용하는 모든 샘플의 길이는 30으로 가정합니다.\n",
        "\n",
        "4. 이진 분류를 수행하는 모델로, 출력층의 뉴런은 1개로 시그모이드 함수를 사용합니다.\n",
        "\n",
        "5. 은닉층은 1개입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef4K36f-_8rR"
      },
      "source": [
        "1. Embedding = input(5000) x embedding (100) => 500000\n",
        "\n",
        "2. Wx = 100(embedding) * 128(hidden) = 12,800\n",
        "\n",
        "3. Wh = 128 * 128 = 16,384\n",
        "\n",
        "4. bias(hidden)  = 128\n",
        "\n",
        "5. Wy = 128\n",
        "bias(output) = 1 \n",
        "\n",
        "=> 총합 529,441"
      ]
    }
  ]
}
